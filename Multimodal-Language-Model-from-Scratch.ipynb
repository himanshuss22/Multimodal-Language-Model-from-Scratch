{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "from typing import List, Tuple, Optional, Dict, Union\n",
    "import json\n",
    "import glob\n",
    "import os\n",
    "from safetensors import safe_open\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "class VisionConfig:\n",
    "    def __init__(self, \n",
    "                 hidden_size=768,\n",
    "                 intermediate_size=3072,\n",
    "                 num_layers=12,\n",
    "                 num_heads=12,\n",
    "                 num_channels=3,\n",
    "                 image_size=224,\n",
    "                 patch_size=16,\n",
    "                 layer_norm_eps=1e-6,\n",
    "                 dropout_rate=0.0):\n",
    "        self.hidden_size = hidden_size\n",
    "        self.intermediate_size = intermediate_size\n",
    "        self.num_layers = num_layers\n",
    "        self.num_heads = num_heads\n",
    "        self.num_channels = num_channels\n",
    "        self.image_size = image_size\n",
    "        self.patch_size = patch_size\n",
    "        self.layer_norm_eps = layer_norm_eps\n",
    "        self.dropout_rate = dropout_rate\n",
    "        self.num_patches = (image_size // patch_size) ** 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "class PatchEmbedder(nn.Module):\n",
    "    \"\"\"Converts images into patch embeddings.\"\"\"\n",
    "    def __init__(self, config: VisionConfig):\n",
    "        super().__init__()\n",
    "        self.patch_conv = nn.Conv2d(\n",
    "            in_channels=config.num_channels,\n",
    "            out_channels=config.hidden_size,\n",
    "            kernel_size=config.patch_size,\n",
    "            stride=config.patch_size,\n",
    "            padding=0  # No padding, patches are non-overlapping\n",
    "        )\n",
    "        self.positional_emb = nn.Embedding(config.num_patches, config.hidden_size)\n",
    "        self.register_buffer(\"pos_ids\", torch.arange(config.num_patches).unsqueeze(0))\n",
    "\n",
    "    def forward(self, images: torch.Tensor) -> torch.Tensor:\n",
    "        # images: [B, C, H, W] -> [B, hidden_size, num_patches_h, num_patches_w]\n",
    "        patches = self.patch_conv(images)\n",
    "        # Flatten patches: [B, hidden_size, num_patches]\n",
    "        embeddings = patches.flatten(2)\n",
    "        # Transpose: [B, num_patches, hidden_size]\n",
    "        embeddings = embeddings.transpose(1, 2)\n",
    "        # Add positional embeddings\n",
    "        embeddings += self.positional_emb(self.pos_ids)\n",
    "        return embeddings\n",
    "\n",
    "class VisionAttention(nn.Module):\n",
    "    \"\"\"Multi-head self-attention for vision transformer.\"\"\"\n",
    "    def __init__(self, config: VisionConfig):\n",
    "        super().__init__()\n",
    "        self.num_heads = config.num_heads\n",
    "        self.head_dim = config.hidden_size // config.num_heads\n",
    "        self.scale = self.head_dim ** -0.5\n",
    "        self.dropout = nn.Dropout(config.dropout_rate)\n",
    "        \n",
    "        self.query = nn.Linear(config.hidden_size, config.hidden_size)\n",
    "        self.key = nn.Linear(config.hidden_size, config.hidden_size)\n",
    "        self.value = nn.Linear(config.hidden_size, config.hidden_size)\n",
    "        self.output = nn.Linear(config.hidden_size, config.hidden_size)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        B, N, D = x.shape\n",
    "        # Linear projections\n",
    "        q = self.query(x).view(B, N, self.num_heads, self.head_dim).transpose(1, 2)\n",
    "        k = self.key(x).view(B, N, self.num_heads, self.head_dim).transpose(1, 2)\n",
    "        v = self.value(x).view(B, N, self.num_heads, self.head_dim).transpose(1, 2)\n",
    "        # Attention scores\n",
    "        scores = (q @ k.transpose(-2, -1)) * self.scale\n",
    "        attn = F.softmax(scores, dim=-1)\n",
    "        attn = self.dropout(attn)\n",
    "        # Attention output\n",
    "        out = (attn @ v).transpose(1, 2).contiguous().view(B, N, D)\n",
    "        return self.output(out)\n",
    "\n",
    "class VisionMLP(nn.Module):\n",
    "    \"\"\"Feed-forward network for vision transformer.\"\"\"\n",
    "    def __init__(self, config: VisionConfig):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(config.hidden_size, config.intermediate_size)\n",
    "        self.fc2 = nn.Linear(config.intermediate_size, config.hidden_size)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        x = F.gelu(self.fc1(x), approximate=\"tanh\")\n",
    "        return self.fc2(x)\n",
    "\n",
    "class VisionEncoderLayer(nn.Module):\n",
    "    \"\"\"Single encoder layer combining attention and MLP.\"\"\"\n",
    "    def __init__(self, config: VisionConfig):\n",
    "        super().__init__()\n",
    "        self.attention = VisionAttention(config)\n",
    "        self.mlp = VisionMLP(config)\n",
    "        self.norm1 = nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps)\n",
    "        self.norm2 = nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        x = x + self.attention(self.norm1(x))\n",
    "        x = x + self.mlp(self.norm2(x))\n",
    "        return x\n",
    "\n",
    "class VisionTransformer(nn.Module):\n",
    "    \"\"\"Complete Siglip vision transformer.\"\"\"\n",
    "    def __init__(self, config: VisionConfig):\n",
    "        super().__init__()\n",
    "        self.embedder = PatchEmbedder(config)\n",
    "        self.layers = nn.ModuleList([VisionEncoderLayer(config) for _ in range(config.num_layers)])\n",
    "        self.final_norm = nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps)\n",
    "\n",
    "    def forward(self, images: torch.Tensor) -> torch.Tensor:\n",
    "        x = self.embedder(images)\n",
    "        for layer in self.layers:\n",
    "            x = layer(x)\n",
    "        return self.final_norm(x)\n",
    "\n",
    "\n",
    "class LanguageConfig:\n",
    "    def __init__(self,\n",
    "                 vocab_size=257152,\n",
    "                 hidden_size=2048,\n",
    "                 intermediate_size=8192,\n",
    "                 num_layers=18,\n",
    "                 num_heads=16,\n",
    "                 num_kv_heads=8,\n",
    "                 head_dim=256,\n",
    "                 max_seq_len=8192,\n",
    "                 rms_norm_eps=1e-6,\n",
    "                 rope_theta=10000.0,\n",
    "                 dropout_rate=0.0):\n",
    "        self.vocab_size = vocab_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.intermediate_size = intermediate_size\n",
    "        self.num_layers = num_layers\n",
    "        self.num_heads = num_heads\n",
    "        self.num_kv_heads = num_kv_heads\n",
    "        self.head_dim = head_dim\n",
    "        self.max_seq_len = max_seq_len\n",
    "        self.rms_norm_eps = rms_norm_eps\n",
    "        self.rope_theta = rope_theta\n",
    "        self.dropout_rate = dropout_rate\n",
    "\n",
    "\n",
    "\n",
    "class RMSNorm(nn.Module):\n",
    "    \"\"\"Root Mean Square normalization as used in Gemma.\"\"\"\n",
    "    def __init__(self, dim: int, eps: float = 1e-6):\n",
    "        super().__init__()\n",
    "        self.eps = eps\n",
    "        self.weight = nn.Parameter(torch.ones(dim))\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        rms = torch.rsqrt(x.pow(2).mean(-1, keepdim=True) + self.eps)\n",
    "        return x * rms * (1.0 + self.weight)\n",
    "\n",
    "class RotaryEmbedding(nn.Module):\n",
    "    \"\"\"Rotary positional embeddings for attention.\"\"\"\n",
    "    def __init__(self, dim: int, max_seq_len: int = 8192, theta: float = 10000.0):\n",
    "        super().__init__()\n",
    "        self.dim = dim\n",
    "        freqs = 1.0 / (theta ** (torch.arange(0, dim, 2).float() / dim))\n",
    "        t = torch.arange(max_seq_len)\n",
    "        freqs = torch.outer(t, freqs).float()\n",
    "        self.register_buffer(\"cos\", freqs.cos())\n",
    "        self.register_buffer(\"sin\", freqs.sin())\n",
    "\n",
    "    def forward(self, x: torch.Tensor, start_pos: int = 0) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "        seq_len = x.shape[-2]\n",
    "        return (self.cos[start_pos:start_pos + seq_len].to(x.device),\n",
    "                self.sin[start_pos:start_pos + seq_len].to(x.device))\n",
    "\n",
    "def apply_rotary_emb(q: torch.Tensor, k: torch.Tensor, cos: torch.Tensor, sin: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "    q1, q2 = q[..., :q.shape[-1] // 2], q[..., q.shape[-1] // 2:]\n",
    "    k1, k2 = k[..., :k.shape[-1] // 2], k[..., k.shape[-1] // 2:]\n",
    "    q_rot = torch.cat([-q2 * sin + q1 * cos, q1 * sin + q2 * cos], dim=-1)\n",
    "    k_rot = torch.cat([-k2 * sin + k1 * cos, k1 * sin + k2 * cos], dim=-1)\n",
    "    return q_rot, k_rot\n",
    "\n",
    "class KVCache:\n",
    "    \"\"\"Key-Value cache for efficient autoregressive generation.\"\"\"\n",
    "    def __init__(self):\n",
    "        self.keys = []\n",
    "        self.values = []\n",
    "\n",
    "    def update(self, key: torch.Tensor, value: torch.Tensor, layer_idx: int) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "        if len(self.keys) <= layer_idx:\n",
    "            self.keys.append(key)\n",
    "            self.values.append(value)\n",
    "        else:\n",
    "            self.keys[layer_idx] = torch.cat([self.keys[layer_idx], key], dim=2)\n",
    "            self.values[layer_idx] = torch.cat([self.values[layer_idx], value], dim=2)\n",
    "        return self.keys[layer_idx], self.values[layer_idx]\n",
    "\n",
    "    def get_seq_len(self) -> int:\n",
    "        return self.keys[0].shape[2] if self.keys else 0\n",
    "\n",
    "class LanguageAttention(nn.Module):\n",
    "    \"\"\"Causal multi-head attention with rotary embeddings and KV cache.\"\"\"\n",
    "    def __init__(self, config: LanguageConfig, layer_idx: int):\n",
    "        super().__init__()\n",
    "        self.num_heads = config.num_heads\n",
    "        self.num_kv_heads = config.num_kv_heads\n",
    "        self.head_dim = config.head_dim\n",
    "        self.scale = self.head_dim ** -0.5\n",
    "        self.layer_idx = layer_idx\n",
    "        self.dropout = nn.Dropout(config.dropout_rate)\n",
    "        self.num_kv_groups = config.num_heads // config.num_kv_heads\n",
    "\n",
    "        self.q_proj = nn.Linear(config.hidden_size, config.num_heads * config.head_dim, bias=False)\n",
    "        self.k_proj = nn.Linear(config.hidden_size, config.num_kv_heads * config.head_dim, bias=False)\n",
    "        self.v_proj = nn.Linear(config.hidden_size, config.num_kv_heads * config.head_dim, bias=False)\n",
    "        self.o_proj = nn.Linear(config.num_heads * config.head_dim, config.hidden_size, bias=False)\n",
    "        self.rotary = RotaryEmbedding(config.head_dim, config.max_seq_len, config.rope_theta)\n",
    "\n",
    "    def forward(self, x: torch.Tensor, mask: torch.Tensor, cache: Optional[KVCache] = None, pos_ids: Optional[torch.Tensor] = None) -> torch.Tensor:\n",
    "        B, T, _ = x.shape\n",
    "        q = self.q_proj(x).view(B, T, self.num_heads, self.head_dim).transpose(1, 2)\n",
    "        k = self.k_proj(x).view(B, T, self.num_kv_heads, self.head_dim).transpose(1, 2)\n",
    "        v = self.v_proj(x).view(B, T, self.num_kv_heads, self.head_dim).transpose(1, 2)\n",
    "\n",
    "        start_pos = cache.get_seq_len() if cache else 0\n",
    "        cos, sin = self.rotary(q, start_pos)\n",
    "        q, k = apply_rotary_emb(q, k, cos, sin)\n",
    "\n",
    "        if cache:\n",
    "            k, v = cache.update(k, v, self.layer_idx)\n",
    "        \n",
    "        # Repeat KV for group query attention\n",
    "        k = k.repeat_interleave(self.num_kv_groups, dim=1)\n",
    "        v = v.repeat_interleave(self.num_kv_groups, dim=1)\n",
    "\n",
    "        scores = (q @ k.transpose(-2, -1)) * self.scale + mask\n",
    "        attn = F.softmax(scores, dim=-1)\n",
    "        attn = self.dropout(attn)\n",
    "        out = (attn @ v).transpose(1, 2).contiguous().view(B, T, -1)\n",
    "        return self.o_proj(out)\n",
    "\n",
    "class LanguageMLP(nn.Module):\n",
    "    \"\"\"Gemma's gated MLP.\"\"\"\n",
    "    def __init__(self, config: LanguageConfig):\n",
    "        super().__init__()\n",
    "        self.gate = nn.Linear(config.hidden_size, config.intermediate_size, bias=False)\n",
    "        self.up = nn.Linear(config.hidden_size, config.intermediate_size, bias=False)\n",
    "        self.down = nn.Linear(config.intermediate_size, config.hidden_size, bias=False)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        return self.down(F.gelu(self.gate(x), approximate=\"tanh\") * self.up(x))\n",
    "\n",
    "class DecoderLayer(nn.Module):\n",
    "    \"\"\"Single decoder layer for Gemma.\"\"\"\n",
    "    def __init__(self, config: LanguageConfig, layer_idx: int):\n",
    "        super().__init__()\n",
    "        self.attn = LanguageAttention(config, layer_idx)\n",
    "        self.mlp = LanguageMLP(config)\n",
    "        self.norm1 = RMSNorm(config.hidden_size, config.rms_norm_eps)\n",
    "        self.norm2 = RMSNorm(config.hidden_size, config.rms_norm_eps)\n",
    "\n",
    "    def forward(self, x: torch.Tensor, mask: torch.Tensor, cache: Optional[KVCache] = None, pos_ids: Optional[torch.Tensor] = None) -> torch.Tensor:\n",
    "        x = x + self.attn(self.norm1(x), mask, cache, pos_ids)\n",
    "        x = x + self.mlp(self.norm2(x))\n",
    "        return x\n",
    "\n",
    "class LanguageModel(nn.Module):\n",
    "    \"\"\"Gemma language model.\"\"\"\n",
    "    def __init__(self, config: LanguageConfig):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        self.embedding = nn.Embedding(config.vocab_size, config.hidden_size)\n",
    "        self.layers = nn.ModuleList([DecoderLayer(config, i) for i in range(config.num_layers)])\n",
    "        self.norm = RMSNorm(config.hidden_size, config.rms_norm_eps)\n",
    "        self.lm_head = nn.Linear(config.hidden_size, config.vocab_size, bias=False)\n",
    "        self.lm_head.weight = self.embedding.weight  # Tie weights\n",
    "\n",
    "    def forward(self, embeddings: torch.Tensor, mask: torch.Tensor, cache: Optional[KVCache] = None, pos_ids: Optional[torch.Tensor] = None) -> Dict[str, torch.Tensor]:\n",
    "        x = embeddings * (self.config.hidden_size ** 0.5)\n",
    "        for layer in self.layers:\n",
    "            x = layer(x, mask, cache, pos_ids)\n",
    "        x = self.norm(x)\n",
    "        logits = self.lm_head(x)\n",
    "        return {\"logits\": logits, \"kv_cache\": cache} if cache else {\"logits\": logits}\n",
    "\n",
    "class MultimodalConfig:\n",
    "    def __init__(self, vision_config: dict, text_config: dict, projection_dim=2048, image_token_id=256000):\n",
    "        self.vision_config = VisionConfig(**vision_config)\n",
    "        self.text_config = LanguageConfig(**text_config)\n",
    "        self.projection_dim = projection_dim\n",
    "        self.image_token_id = image_token_id\n",
    "        self.hidden_size = self.text_config.hidden_size\n",
    "\n",
    "class Projector(nn.Module):\n",
    "    \"\"\"Projects vision features to language space.\"\"\"\n",
    "    def __init__(self, in_dim: int, out_dim: int):\n",
    "        super().__init__()\n",
    "        self.proj = nn.Linear(in_dim, out_dim, bias=True)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        return self.proj(x)\n",
    "\n",
    "class PaliGemma(nn.Module):\n",
    "    \"\"\"PaliGemma: Combines vision and language models.\"\"\"\n",
    "    def __init__(self, config: MultimodalConfig):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        self.vision = VisionTransformer(config.vision_config)\n",
    "        self.projector = Projector(config.vision_config.hidden_size, config.projection_dim)\n",
    "        self.language = LanguageModel(config.text_config)\n",
    "\n",
    "    def merge_inputs(self, input_ids: torch.Tensor, image_features: torch.Tensor, embeddings: torch.Tensor, mask: torch.Tensor, cache: Optional[KVCache] = None) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor]:\n",
    "        B, T = input_ids.shape\n",
    "        image_features = image_features / (self.config.hidden_size ** 0.5)\n",
    "        final_emb = torch.zeros(B, T, self.config.hidden_size, device=embeddings.device, dtype=embeddings.dtype)\n",
    "        \n",
    "        text_mask = (input_ids != self.config.image_token_id).unsqueeze(-1)\n",
    "        image_mask = (input_ids == self.config.image_token_id).unsqueeze(-1)\n",
    "        \n",
    "        final_emb = torch.where(text_mask, embeddings, final_emb)\n",
    "        final_emb = final_emb.masked_scatter(image_mask, image_features)\n",
    "\n",
    "        # Causal mask\n",
    "        q_len = T\n",
    "        kv_len = cache.get_seq_len() + q_len if cache else q_len\n",
    "        causal_mask = torch.zeros(B, 1, q_len, kv_len, device=embeddings.device, dtype=embeddings.dtype)\n",
    "        \n",
    "        pos_ids = mask.cumsum(-1).masked_fill_(mask == 0, 1) if cache is None else mask.cumsum(-1)[:, -1].unsqueeze(0)\n",
    "        return final_emb, causal_mask, pos_ids\n",
    "\n",
    "    def forward(self, input_ids: torch.Tensor, pixel_values: torch.Tensor, mask: torch.Tensor, cache: Optional[KVCache] = None) -> Dict[str, torch.Tensor]:\n",
    "        embeddings = self.language.embedding(input_ids)\n",
    "        image_features = self.projector(self.vision(pixel_values))\n",
    "        inputs, causal_mask, pos_ids = self.merge_inputs(input_ids, image_features, embeddings, mask, cache)\n",
    "        return self.language(inputs, causal_mask, cache, pos_ids)\n",
    "    \n",
    "class MultimodalProcessor:\n",
    "    def __init__(self, tokenizer, image_size: int, num_image_tokens: int):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.image_size = image_size\n",
    "        self.num_image_tokens = num_image_tokens\n",
    "        self.image_token = \"<image>\"\n",
    "        self.tokenizer.add_tokens([self.image_token] + [f\"<loc{i:04d}>\" for i in range(1024)] + [f\"<seg{i:03d}>\" for i in range(128)])\n",
    "        self.image_token_id = self.tokenizer.convert_tokens_to_ids(self.image_token)\n",
    "\n",
    "    def process_image(self, image: Image.Image) -> torch.Tensor:\n",
    "        img = image.resize((self.image_size, self.image_size), Image.Resampling.BICUBIC)\n",
    "        img = np.array(img).astype(np.float32) / 255.0\n",
    "        img = (img - np.array([0.5, 0.5, 0.5])) / np.array([0.5, 0.5, 0.5])\n",
    "        img = torch.tensor(img.transpose(2, 0, 1)).unsqueeze(0)\n",
    "        return img\n",
    "\n",
    "    def __call__(self, text: str, image: Image.Image) -> Dict[str, torch.Tensor]:\n",
    "        prompt = f\"{self.image_token * self.num_image_tokens}{self.tokenizer.bos_token}{text}\\n\"\n",
    "        inputs = self.tokenizer(prompt, return_tensors=\"pt\", padding=\"longest\", truncation=True)\n",
    "        pixel_values = self.process_image(image)\n",
    "        return {\"input_ids\": inputs[\"input_ids\"], \"attention_mask\": inputs[\"attention_mask\"], \"pixel_values\": pixel_values}\n",
    "\n",
    "\n",
    "\n",
    "def load_model(model_path: str, device: str) -> Tuple[PaliGemma, MultimodalProcessor]:\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_path, padding_side=\"right\")\n",
    "    with open(os.path.join(model_path, \"config.json\"), \"r\") as f:\n",
    "        config_dict = json.load(f)\n",
    "    config = MultimodalConfig(config_dict[\"vision_config\"], config_dict[\"text_config\"])\n",
    "    \n",
    "    model = PaliGemma(config).to(device)\n",
    "    tensors = {}\n",
    "    for file in glob.glob(os.path.join(model_path, \"*.safetensors\")):\n",
    "        with safe_open(file, framework=\"pt\", device=\"cpu\") as f:\n",
    "            for k in f.keys():\n",
    "                tensors[k] = f.get_tensor(k)\n",
    "    model.load_state_dict(tensors, strict=False)\n",
    "    \n",
    "    processor = MultimodalProcessor(tokenizer, config.vision_config.image_size, config.vision_config.num_patches)\n",
    "    return model, processor\n",
    "\n",
    "\n",
    "\n",
    "def generate_text(model: PaliGemma, processor: MultimodalProcessor, prompt: str, image_path: str, max_tokens: int = 100, temp: float = 0.8, top_p: float = 0.9) -> str:\n",
    "    image = Image.open(image_path).convert(\"RGB\")\n",
    "    inputs = processor(prompt, image)\n",
    "    inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "    \n",
    "    cache = KVCache()\n",
    "    generated = []\n",
    "    input_ids = inputs[\"input_ids\"]\n",
    "    mask = inputs[\"attention_mask\"]\n",
    "    pixel_values = inputs[\"pixel_values\"]\n",
    "    eos_id = processor.tokenizer.eos_token_id\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for _ in range(max_tokens):\n",
    "            outputs = model(input_ids, pixel_values, mask, cache)\n",
    "            logits = outputs[\"logits\"][:, -1, :] / temp\n",
    "            probs = F.softmax(logits, dim=-1)\n",
    "            probs_sorted, idxs = probs.sort(dim=-1, descending=True)\n",
    "            cumsum = probs_sorted.cumsum(dim=-1)\n",
    "            mask = (cumsum - probs_sorted) > top_p\n",
    "            probs_sorted[mask] = 0\n",
    "            probs_sorted /= probs_sorted.sum(dim=-1, keepdim=True)\n",
    "            next_token = torch.multinomial(probs_sorted, 1)\n",
    "            next_token = torch.gather(idxs, -1, next_token)\n",
    "            \n",
    "            generated.append(next_token.item())\n",
    "            if next_token.item() == eos_id:\n",
    "                break\n",
    "            input_ids = next_token.unsqueeze(0)\n",
    "            mask = torch.ones_like(input_ids)\n",
    "    \n",
    "    return processor.tokenizer.decode(generated, skip_special_tokens=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "model_path = \"./model\"\n",
    "image_path = \"./bird.jpg\"\n",
    "prompt = \"Describe this image.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "model, processor = load_model(model_path, device)\n",
    "output = generate_text(model, processor, prompt, image_path)\n",
    "print(f\"Prompt: {prompt}\")\n",
    "print(f\"Generated: {output}\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
